{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Akshya Kumar Shrestha\n",
    "# Date: February 10, 2019\n",
    "# Description: Naive Bayes Classifier is used to detect sarcasm on the news headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)\n",
    "raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = raw\n",
    "df.pop('article_link')\n",
    "df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['headline']\n",
    "y = df['is_sarcastic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "nltk.download()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stop_words =  stopwords.words('english') + list(string.punctuation)\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "model = naive_bayes.MultinomialNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_data = vectorizer.transform(X_test)\n",
    "y_predict = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8040059902658181\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today is sunday -> Non Sarcastic\n",
      "youre tall as a giant dwarf -> Sarcastic\n",
      "former versace store clerk sues over secret  -> Non Sarcastic\n",
      "youre very nice little pumpkin ! -> Sarcastic\n",
      "you are chepal -> Non Sarcastic\n",
      "you are rozeen -> Non Sarcastic\n",
      "i am akshya and i am tall -> Sarcastic\n",
      "I work 40 hours a week for us to be this poor -> Sarcastic\n",
      "i am shristi and i am sharma -> Non Sarcastic\n",
      "That speaker was so interesting that I barely needed to drink my third cup of coffee. -> Sarcastic\n",
      "I am Chepal and im a guitarist -> Non Sarcastic\n",
      "i am akshya and i am a freaking bollywood star -> Non Sarcastic\n",
      "I am little shristi and brand new miss nepal -> Non Sarcastic\n",
      "i promise me and chepal will finish fyp today.. yayy -> Sarcastic\n",
      "i am rozeen and i am best programmer in the world yayy yayy yayy -> Non Sarcastic\n",
      "i am rozeen and i am better and richer than bill gates -> Non Sarcastic\n",
      "amazon is my father business -> Non Sarcastic\n",
      "thanks for ruining the day -> Non Sarcastic\n",
      "i am chepal and i am better than rozeen -> Non Sarcastic\n",
      "i am rozeen and better than chepal -> Non Sarcastic\n",
      "Nice perfume. How long did you marinate in it? -> Sarcastic\n"
     ]
    }
   ],
   "source": [
    "sample_data = ['today is sunday', \n",
    "               'youre tall as a giant dwarf', \n",
    "               'former versace store clerk sues over secret ',\n",
    "               'youre very nice little pumpkin !',\n",
    "              'you are chepal',\n",
    "              'you are rozeen',\n",
    "              'i am akshya and i am tall',\n",
    "              'I work 40 hours a week for us to be this poor',\n",
    "              'i am shristi and i am sharma',\n",
    "              'That speaker was so interesting that I barely needed to drink my third cup of coffee.',\n",
    "              'I am Chepal and im a guitarist',\n",
    "              'i am akshya and i am a freaking bollywood star',\n",
    "              'I am little shristi and brand new miss nepal',\n",
    "              'i promise me and chepal will finish fyp today.. yayy',\n",
    "              'i am rozeen and i am best programmer in the world yayy yayy yayy',\n",
    "              'i am rozeen and i am better and richer than bill gates',\n",
    "              'amazon is my father business',\n",
    "              'thanks for ruining the day',\n",
    "              'i am chepal and i am better than rozeen',\n",
    "              'i am rozeen and better than chepal',\n",
    "              'Nice perfume. How long did you marinate in it?']\n",
    "predict_sample_data = vectorizer.transform(sample_data)\n",
    "predicted = model.predict(predict_sample_data)\n",
    "\n",
    "for i in range(0, len(sample_data)):\n",
    "    if predicted[i] == 1:\n",
    "        print(sample_data[i], \"-> Sarcastic\")\n",
    "    else:\n",
    "        print(sample_data[i], \"-> Non Sarcastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
